{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca90c08d-023d-4370-a8e1-0bf6694ea15a",
   "metadata": {},
   "source": [
    "# Learn RAG From Scratch - Python AI from a LangChain Engineer\n",
    "## 17/04/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c91ed-47cf-419a-8e8a-6aa73afe67c6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ADD8E6; padding: 10px; border: 1px solid #ADD8E6\">\n",
    "    \n",
    "# Table of Contents\n",
    "## Overview\n",
    "## Basics\n",
    "* [Indexing](#indexing)\n",
    "* [Retrieval](#retrieval)\n",
    "* [Generation](#generation)\n",
    "\n",
    "## Advanced\n",
    "* [Query Translation (Multi Query)](#query-translation-multi-query)\n",
    "* [Query Translation (RAG Fusion)](#query-translation-rag-fusion)\n",
    "* [Query Translation (Decomposition)](#query-translation-decomposition)\n",
    "* [Query Translation (Step Back)](#query-translation-step-back)\n",
    "* [Query Translation (HyDE)](#query-translation-hyde)\n",
    "* [Routing](#routing)\n",
    "* [Query Construction](#query-construction)\n",
    "* [Indexing (Multi Representation)](#indexing-multi-representation)\n",
    "* [Indexing (RAPTOR)](#indexing-raptor)\n",
    "* [Indexing (ColBERT)](#indexing-colbert)\n",
    "* [CRAG](#crag)\n",
    "* [Adaptive RAG](#adaptive-rag)\n",
    "* [The Future of RAG](#the-future-of-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab84d9a-25d9-4af4-96f3-14a6dd6a5442",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066a26b-cd55-4341-afd0-5d718220e180",
   "metadata": {},
   "source": [
    "In this course Lance Martin will teach you how to implement RAG from scratch. Lance is a software engineer at LangChain, and LangChain is one of the most common ways to implement RAG. Lance will help you understand how to use RAG to combine custom data with LLMs.\n",
    "\n",
    "I’m going to show you a short course focused on RAG (Retrieval Augmented Generation) which is one of the most popular kind of ideas in applications in LLMs today. So, really the motivation for this is that most of the world’s data is private, whereas LLMs are trained on publicly available data.\n",
    "\n",
    "Note: more than 95% of the world's data is \"private\", but we can \"feed it\" to LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c93d02-bc65-4d35-884f-1a6add333337",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG2.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 1</b>: Comparison of LLMs / Pre-training VS Context Window (Number of Tokens)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa3c9e5-0708-4459-a2aa-ca7226f3df04",
   "metadata": {},
   "source": [
    "Of interest:\n",
    "\n",
    "https://huggingface.co/blog/mixtral\n",
    "\n",
    "https://x.com/RihardJarc/status/1778082161595208124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cedab-4906-4186-ad66-69718b05a136",
   "metadata": {},
   "source": [
    "So, you can kind of see on the bottom of the x-axis the number of tokens used for pre-training various LLMs, so it can of varies from 1.5 trillion tokens in the case of smaller models like Phi-2, out to some very large number for proprietary models like GPT4 and Claude 3. But what’s really interesting is that the context window or the ability to feed external information into these LLMs is actually getting larger, so about 1 year ago, context window was between 4 and 8.000 tokens, you know that’s like maybe a dozen pages of text.\n",
    "\n",
    "We’ve recently seen models all the way out to a million tokens which is thousands of pages of text, so while these LLMs are trained on large scale public data, it’s increasingly feasible to feed them, this huge mass of private data that they’ve never seen, that private data can be your of personal data, it can be corporate data, or other information that you want to pass to an LLM that’s not natively in this training set. \n",
    "\n",
    "So, that is the main motivation for RAG, that’s really the idea that LLMs; one, they are the center of a new kind of operating system; and two, it’s increasingly critical to be able to feed information from external sources such as private data into LLMs for processing, so that’s kind of the overarching motivation for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde1da5-dd0d-484b-bdfc-95e0327e8b69",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG3.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 2</b>: Connecting LLM to external data is a central need</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930a2ce-e16b-4184-8f5d-069f8fcbd9fb",
   "metadata": {},
   "source": [
    "Of interest:\n",
    "\n",
    "https://x.com/karpathy/status/1707437820045062561"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f0e8c-cf55-4eee-98df-3c1d42fcc34a",
   "metadata": {},
   "source": [
    "RAG refers to Retrieval Augmented Generation, you can think of it in three very general steps: there’s a process of indexing of external data, so you can think about this as building a database. Many companies already have large scale databases in different forms, they could be SQL DBS, relational DBS, or they could be vector stores, but the point is that documents are indexed, such that they can be retrieved based upon some heuristics relative to an input like a question, and those relevant documents can be passed to an LLM, and the LLM can produce answers that are grounded in that retrieved information, so that’s kind of the centerpiece or central idea behind RAG and why it’s really powerful technology, because it’s really uniting the knowledge and processing capacity of LLMs with large scale private external data source for which most of the important data in the world still lives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2c287-02ca-451e-8b97-3e323b1d0d10",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"/RAG4.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 3</b>: Retrieval Augmented Generation (RAG) General Diagram Flow</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44929518-2cc6-4771-913a-9be8e15c88e3",
   "metadata": {},
   "source": [
    "Next, we’re going to kind of build up a complete understanding of the RAG landscape and covering a bunch of interesting papers and techniques that explain kind of how to do RAG, and I’ve really broken it down into a few different sections:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c0866-d2c4-4b81-9f45-cd7646a0caf7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG1.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 4</b>: General Workflow of RAG with LangChain</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65e454-0f97-4eec-803e-50ae32a3bd18",
   "metadata": {},
   "source": [
    "- So, starting with a question on the left, the first kind of section is what I call Query Translation, so this captures a bunch of different methods to take a question from a user and modify it in some way to make it better suited for retrieval from one of these indexes we’ve talked about, that can use methods like query writing, it can be decomposing the query into constituent sub-questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4693e9-0f7a-4ef4-9287-24e4bcc56495",
   "metadata": {},
   "source": [
    "- Then, there’s a question of Routing, so, taking that decomposed re-written question and routing it to the right place, you might have Multiple Vector Stores, a Relational DB, a Graph DB, and a Vector Store, so, it’s the challenge of getting a question to the right source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f38ffb-f995-46f2-b223-8ce7e7caba8c",
   "metadata": {},
   "source": [
    "- Then, there’s kind of challenge of Query Construction which is basically taking Natural Language and converting it into the DSL (Domain Specific Language) necessary for whatever data source you want to work with, a classic example here is Text-to-SQL which is kind of a very well-studied process, but, Text-to-Cypher for Graph-DB is very interesting, Text-to-Metadata Filters for Vector-DBs is also a very big area of Study.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e0422-4b1d-4339-af4a-5c3ebb89fdbd",
   "metadata": {},
   "source": [
    "- Then, there’s Indexing, so that’s the process of taking your documents and processing in some way they can be easily retrieved and there’s a bunch of techniques for that we’ll talk through, we’ll talk through different embedding methods, we’ll talk about different indexing strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3889f-c4d3-4b0c-97bb-3957f2e0f53b",
   "metadata": {},
   "source": [
    "- After, Retrieval, there are different techniques to re-rank or filter retrieved-documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63db701-9cc6-43b5-92aa-7b0a6c620c00",
   "metadata": {},
   "source": [
    "- And then finally, we’ll talk about Generation, a kind of interesting new set of methods to do what we might call as Active RAG, so, in that Retrieval or Generation Stage, grade documents, grade answers, grade for relevance to the questions, grade for faithfulness to the documents, I.E check for hallucinations, and if either fail, feedback; re-retrieve or rewrite the question, re-generate the answer so forth, so there’s a really interesting set of methods we’re going to talk through that cover that like retrieval and generation with feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d31b9-6fa5-4ce3-9b18-923db75d7c69",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae1088-7ca6-4764-bd8f-3c93b0cb9379",
   "metadata": {},
   "source": [
    "In terms of general outline, we’ll cover the basics first, it’ll go through Indexing, Retrieval, and Generation kind of in the bare bones, and then we’ll talk through more advanced techniques that we just saw on the prior slide:  Query Transformations, Routing, Query Construction, and so forth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98727192-89c0-4f8f-add1-9082eb0a0005",
   "metadata": {},
   "source": [
    "### <a id=\"indexing\"></a>Indexing\n",
    "\n",
    "So, in the previous section we saw the main overall components of RAG pipelines: Indexing, Retrieval, and Generation, and here we’re going to kind of deep dive on indexing and give a quick overview of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ceb18-e1ab-4b6e-9c86-ba5e1447929d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG8.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 5</b>: Indexing Stage</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb86d7-bff3-405a-8d50-91780dc985fd",
   "metadata": {},
   "source": [
    "The first aspect of indexing is we have some external documents that we actually want to load and put into what we’re trying to call Retriever, and the goal of this Retriever is simply given an input question I want to fish out documents that are related to my question in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75f232-65a5-476c-a860-1ee91e370c63",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG11.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 6</b>: Document Loading</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdfef1-2790-4ca3-87d5-21b4ffa7bc27",
   "metadata": {},
   "source": [
    "Now, the way to establish that relationship, or relevance, or similarity is typically done using some kind of numerical representation of documents, and the reason is that is very easy to compare vectors, for example, of numbers relative to just free form text, and so, a lot of approaches have been developed over the years to take text documents and compress them down into a numerical representation that then can be very easily searched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d44d1-b06d-4155-811b-72cdaba26ebb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG12.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 7</b>: Numerical Representation for Search</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5ed37-4a6b-4f3f-bae9-2f9f1bda7d58",
   "metadata": {},
   "source": [
    "There’s a few ways to do that, so Google and others came up with many interesting statistical where you take a document, look at the frequency of words, and you build what they call Sparse Vectors, such that the vector locations are a large vocabulary of possible words, each value represents the number of occurrences of that particular word, and it’s sparse because there’s of course many zeros, it’s a very large vocabulary relative to what’s present in the document, and there’s very good search methods over this type of numerical representation. Now, a bit more recently embedding methods that are machine learned, so you take a document and you build a compressed fixed length representation of that document have been developed with correspondingly very strong search methods over embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e937461-7baa-47ee-b785-ac59394d56a9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG13.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 8</b>: Statistical and Machhine Learned Representations</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cb173-eedb-4095-8b4a-9680fcc42661",
   "metadata": {},
   "source": [
    "So, the intuition here is that we take documents, and we typically split them because embedding models actually have limited Context Windows, so, in the order of maybe 512 tokens, up to 8.000 tokens or beyond, but they are nor infinitely large, so documents are split, and each document is compressed into a vector, and that vector captures a semantic meaning of the document itself. The questions are indexed vectors that can be embedded in the exactly same, and the numerical comparison in some form, using very different types of methods can be performed on these vectors to fish out relevant documents relative to my question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a41c5-fe04-4882-8248-911d82843f2c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG14.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 9</b>: Loading, Splitting and Embedding (Index makes documents easy to retrieve)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c460e-e731-4e7b-9c3b-144a4dd07872",
   "metadata": {},
   "source": [
    "## RAG From Scratch: Parts 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64109e7c-93d7-4c0f-852c-a9476acdac17",
   "metadata": {},
   "source": [
    "Let’s just do a quick code walk through on some of these points. I’ve installed here some packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f39a9b-743c-4823-a61b-0fab850c3da8",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5de626-66c0-4425-a3e0-34361ed4be53",
   "metadata": {},
   "source": [
    "(1) Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcc8f0d-224c-4109-a622-8cdab61e6c7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "     -------------------------------------- 884.2/884.2 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.2-py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 54.4/54.4 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "     -------------------------------------- 611.1/611.1 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.16-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting langsmith<0.4,>=0.1.125\n",
      "  Downloading langsmith-0.3.2-py3-none-any.whl (333 kB)\n",
      "     -------------------------------------- 333.0/333.0 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.32\n",
      "  Downloading langchain_core-0.3.32-py3-none-any.whl (412 kB)\n",
      "     -------------------------------------- 412.4/412.4 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (2.31.0)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (1.23.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (6.0)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-win_amd64.whl (442 kB)\n",
      "     -------------------------------------- 442.0/442.0 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (1.4.39)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Collecting openai<2.0.0,>=1.58.1\n",
      "  Downloading openai-1.60.2-py3-none-any.whl (456 kB)\n",
      "     -------------------------------------- 456.1/456.1 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting packaging<25,>=23.2\n",
      "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 67.3/67.3 kB 1.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting typing_extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting tqdm>=4.65.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting orjson>=3.9.12\n",
      "  Downloading orjson-3.10.15-cp310-cp310-win_amd64.whl (133 kB)\n",
      "     -------------------------------------- 133.6/133.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Collecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.3/64.3 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.3/62.3 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.8/94.8 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-32.0.0-py2.py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting chroma-hnswlib==0.7.6\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "     -------------------------------------- 151.0/151.0 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "     ---------------------------------------- 11.3/11.3 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting pydantic>=1.9\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "     -------------------------------------- 431.7/431.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-5.1.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "     -------------------------------------- 242.4/242.4 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "     -------------------------------------- 118.1/118.1 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting httpx>=0.27.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 73.5/73.5 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting build>=1.0.3\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-3.11.0-py2.py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 72.0/72.0 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.1/153.1 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.70.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 2.9 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3\n",
      "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "     ---------------------------------------- 90.5/90.5 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "     ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (22.1.0)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.4/44.4 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting pyproject_hooks\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.8/50.8 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting starlette<0.46.0,>=0.40.0\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.5/71.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: idna in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: anyio in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2023.11.17)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting durationpy>=0.7\n",
      "  Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "     -------------------------------------- 210.8/210.8 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 54.5/54.5 kB ? eta 0:00:00\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-win_amd64.whl (495 kB)\n",
      "     -------------------------------------- 495.5/495.5 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "     -------------------------------------- 434.5/434.5 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.8.2-cp310-cp310-win_amd64.whl (204 kB)\n",
      "     -------------------------------------- 204.4/204.4 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.2.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "     -------------------------------------- 221.7/221.7 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-proto==1.29.0\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "     -------------------------------------- 166.6/166.6 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (2.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl (464 kB)\n",
      "     -------------------------------------- 464.1/464.1 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-win_amd64.whl (284 kB)\n",
      "     -------------------------------------- 284.1/284.1 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-14.2-cp310-cp310-win_amd64.whl (164 kB)\n",
      "     -------------------------------------- 164.4/164.4 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting httptools>=0.6.3\n",
      "  Downloading httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "     ---------------------------------------- 88.3/88.3 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "     -------------------------------------- 183.9/183.9 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.9.0)\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.0/96.0 kB ? eta 0:00:00\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (0.4.3)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.2.1)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.2/83.2 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53885 sha256=24ae971ad4c682cd7acdce99a267bc4e6d9c1a1bd1eabcfbd0d9fac22968c421\n",
      "  Stored in directory: c:\\users\\daniel\\appdata\\local\\pip\\cache\\wheels\\c4\\41\\b6\\f76a356f0791da799545c23894ceae842eeff054d0eb1fb626\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, zstandard, zipp, websockets, typing_extensions, types-requests, tqdm, tenacity, shellingham, rsa, python-dotenv, pyreadline3, pyproject_hooks, pygments, protobuf, propcache, packaging, overrides, orjson, opentelemetry-util-http, oauthlib, mmh3, mdurl, jsonpointer, jiter, importlib-resources, httpx-sse, httptools, h11, grpcio, fsspec, frozenlist, exceptiongroup, distro, deprecated, chroma-hnswlib, cachetools, bcrypt, backoff, async-timeout, annotated-types, aiohappyeyeballs, uvicorn, typing-inspect, tiktoken, requests-toolbelt, requests-oauthlib, pydantic-core, posthog, opentelemetry-proto, multidict, marshmallow, markdown-it-py, langchainhub, jsonpatch, importlib-metadata, humanfriendly, huggingface-hub, httpcore, googleapis-common-protos, google-auth, build, asgiref, anyio, aiosignal, yarl, watchfiles, tokenizers, starlette, rich, pydantic, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, kubernetes, httpx, dataclasses-json, coloredlogs, typer, pydantic-settings, opentelemetry-semantic-conventions, openai, onnxruntime, langsmith, fastapi, aiohttp, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, opentelemetry-instrumentation-fastapi, langchain, langchain_community, chromadb\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Daniel\\\\anaconda3\\\\Lib\\\\site-packages\\\\~standard\\\\backend_c.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe05b0-4aa7-46b5-9606-9209eb6132e3",
   "metadata": {},
   "source": [
    "I’ve set a few API keys for LangSmith which are very useful for tracing which we’ll see shortly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59eff2-9507-4d43-90fa-1fef7093c53f",
   "metadata": {},
   "source": [
    "(2) LangSmith\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e7a71-47fc-4c99-8119-5872536f4146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = <your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557dc84-664f-47ff-b1da-ef8dc825c34f",
   "metadata": {},
   "source": [
    "(3) API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62086a4f-3793-4e81-a2d8-d3052f087153",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = <your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d1bcb-6ecc-4518-bd3f-cf43bcd95004",
   "metadata": {},
   "source": [
    "### Part 1: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083e245-a8ab-4b56-bea0-2925a72edcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "#Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")        \n",
    "        )    \n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "#Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "#Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "#LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd270d-0e58-4fb5-abd2-4c2accb12577",
   "metadata": {},
   "source": [
    "#### Codeium Detailed Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00916d16-8c6f-4d2e-8af5-a40cc926296e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Importing Libraries\n",
    "\n",
    "The code starts by importing various libraries:\n",
    "\n",
    "- bs4: a Python library for parsing HTML and XML documents.\n",
    "- langchain: a library for building and interacting with large language models.\n",
    "- langchain.text_splitter: a module for splitting text into chunks.\n",
    "- langchain_community.document_loaders: a module for loading documents from various sources.\n",
    "- langchain_community.vectorstores: a module for storing and retrieving vector embeddings.\n",
    "- langchain_core.output_parsers: a module for parsing output from language models.\n",
    "- langchain_core.runnables: a module for defining runnable tasks.\n",
    "- langchain_openai: a module for interacting with OpenAI's language models.\n",
    "\n",
    "\n",
    "Indexing\n",
    "\n",
    "The code then defines an indexing pipeline, which consists of the following steps:\n",
    "\n",
    "1. Loading Documents: The code uses the WebBaseLoader class to load documents from a specified URL. In this case, the URL is a GitHub page containing a blog post. The bs_kwargs parameter is used to specify the HTML elements to parse from the page. In this case, the code is interested in the post-content, post-title, and post-header elements.\n",
    "2. Splitting Text: The code uses the RecursiveCharacterTextSplitter class to split the loaded documents into chunks. This is done to prepare the text for embedding and retrieval.\n",
    "\n",
    "\n",
    "RecursiveCharacterTextSplitter\n",
    "\n",
    "The RecursiveCharacterTextSplitter class is a text splitter that splits text into chunks based on a specified chunk size and overlap. The hyperparameters for this class are:\n",
    "\n",
    "- chunk_size: The maximum size of each chunk in characters. In this case, the chunk size is set to 1000 characters.\n",
    "- chunk_overlap: The amount of overlap between chunks in characters. In this case, the chunk overlap is set to 200 characters.\n",
    "\n",
    "The chunk size and overlap are used to determine the boundaries of each chunk. The splitter will split the text into chunks of size chunk_size, but will also include chunk_overlap characters from the previous chunk in the next chunk. This is done to ensure that the chunks are not too small and to preserve context between chunks.\n",
    "\n",
    "How to Change Chunk Size and Overlap\n",
    "\n",
    "The choice of chunk size and overlap depends on the specific use case and the characteristics of the text data. Here are some general guidelines:\n",
    "\n",
    "- Chunk Size: A larger chunk size can lead to better performance, but may also result in larger embeddings and slower retrieval times. A smaller chunk size can lead to faster retrieval times, but may also result in worse performance due to lack of context. A good starting point for chunk size is between 500-2000 characters.\n",
    "- Chunk Overlap: A larger chunk overlap can help preserve context between chunks, but may also result in more redundant information being stored. A smaller chunk overlap can lead to less redundant information, but may also result in worse performance due to lack of context. A good starting point for chunk overlap is between 100-500 characters.\n",
    "\n",
    "For example, if you have a large corpus of text and want to optimize for performance, you may want to increase the chunk size to 2000 characters and decrease the chunk overlap to 100 characters. On the other hand, if you have a small corpus of text and want to optimize for accuracy, you may want to decrease the chunk size to 500 characters and increase the chunk overlap to 500 characters.\n",
    "\n",
    "Embedding and Retrieval\n",
    "\n",
    "The code then defines an embedding and retrieval pipeline, which consists of the following steps:\n",
    "\n",
    "1. Embedding: The code uses the Chroma class to create a vector store from the split documents. The OpenAIEmbeddings class is used to generate embeddings for each chunk.\n",
    "2. Retrieval: The code uses the as_retriever method to create a retriever object from the vector store.\n",
    "\n",
    "Retrieval and Generation\n",
    "\n",
    "The code then defines a retrieval and generation pipeline, which consists of the following steps:\n",
    "\n",
    "1. Prompt: The code uses the hub.pull method to retrieve a prompt from a remote repository.\n",
    "2. LLM: The code uses the ChatOpenAI class to create a language model object.\n",
    "3. Post-processing: The code defines a format_docs function to format the retrieved documents.\n",
    "4. Chain: The code defines a chain of tasks that consists of the retriever, prompt, language model, and post-processing function.\n",
    "\n",
    "Invocation\n",
    "\n",
    "Finally, the code invokes the chain with a question \"What is Task Decomposition?\" and prints the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1751479-ed00-4cdf-a40a-22d59f5239b0",
   "metadata": {},
   "source": [
    "### Part 2: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8ef91-32ef-49ed-8222-fdf66c19c1d9",
   "metadata": {},
   "source": [
    "And here what I’ll do is deep diving a little bit more on indexing and take a question and a document, and first, I’m just going to compute the number of tokens in for example the question, and this is interesting because embedding models in LLMs more generally operate on tokens, and so, it’s nice to know how large the documents are that I’m trying to feed in, in this case it’s obviously a very small in this case question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284fb2b-b1b7-4476-8f43-58ea4019d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31153c1-2692-4fdb-b0dd-10e535f37ee8",
   "metadata": {},
   "source": [
    "[Count tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considering [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2591dd-eb16-440e-8ea8-f67b4eb87c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e0a3a-1d94-4300-92dd-ef9f822ba9f5",
   "metadata": {},
   "source": [
    "Now, OpenAIEmbeddings is going to be specified, this is where the embedding model is specified, and I just pass my question and my document to 'embed.embed_query' and what you can see here when it runs is that this is mapped to a vector of length 1536, and that fixed length vector representation will be computed for both, and really for any document, so, you are always computing this fix length that encodes the semantics of the text that you've passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cbe44-a898-48d4-977c-20b372f3e159",
   "metadata": {},
   "source": [
    "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a4c28-3ed8-4b49-816a-d980ce66aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a23a2-434f-423b-97af-3fc766efb157",
   "metadata": {},
   "source": [
    "Now I can do things like Cosine Similarity to compare them. [Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical) for OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb8ced-0dc1-403c-b891-0b2d8b214a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10320c9a-7f30-4e4c-8a38-1c1fbcef4c3b",
   "metadata": {},
   "source": [
    "And as we'll see here, some documents can be loaded, this is just like we saw previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a68cd1a-c2ba-48d8-9b52-d546bd273958",
   "metadata": {},
   "source": [
    "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d04b67-6936-469f-9dcf-6ec062d67a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load Block\n",
    "import bs4 \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")        \n",
    "        )    \n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f536f6-bc19-4358-88fb-cf6ed38d347d",
   "metadata": {},
   "source": [
    "It can be splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0143046-bcdb-4332-85d8-f790819f8eb3",
   "metadata": {},
   "source": [
    "[Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
    "\n",
    "> This text splitter is the recommended one for generic text. It tries to split on them in order until the chunnks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cf389-faa7-4fa5-9d15-fb0359eac20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300\n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804274e5-ca13-44f6-b166-a0a2739cbe24",
   "metadata": {},
   "source": [
    "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c254f55-047c-4429-bc52-6310b07e157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                   embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7a467-5d80-440a-a5d7-92bb88e564cd",
   "metadata": {},
   "source": [
    "### <a id=\"retrieval\"></a>Retrieval\n",
    "\n",
    "Next, we'll see how to actually do Retrieval by using this Vector Store. Previously I outlined Indexing and gave an overview of this flow which starts with indexing of our documents, retrieval documents relevant to our question and then generation of answers based on the retrieved documents. \n",
    "\n",
    "We saw in the indexing process, basically it makes documents easy to retrieve and it goes through a flow that basically looks like this: it takes our documents, it splits them in some way into these smaller chunks that can be easily embedded, those embeddings are then numerical representations of those documents that are easily searchable and they are stored in an index, when given a question that's also embedded the index performs a similarity search and returns splits that are relevant to the question. (See Fig 9.). \n",
    "\n",
    "Now, if we dig a little bit more under the hood, we can think about it like this: when we take a document and embed it, let's imagine that embedding just had three dimensions, so, each document is projected into some point in this 3D space:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0f5c7-394c-43cc-b298-b1b639e1cc00",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG15.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 10</b>: Retrieval Powered via Similarity Search</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12f27b-74de-4ba8-b1ad-1d1e234cfec7",
   "metadata": {},
   "source": [
    "Now, the point is that the location in space is determined by the semantic meaning or content in that document, so, to follow that documents in similar locations in space contain similar semantic information, and this very simple idea is really the cornerstone for a lot of search and retrieval methods that you'll see with modern Vector Stores. So, in particular, we take our documents, we embed them into this, in this case a toy 3D space, we take our question do the same, we can then do a search, like a local neighborhood search. You can think about it in this 3D space around our question to say: \"Hey! What documents are nearby?\" and these nearby neighbors are then retrieved because they have similar semantics relative to our question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09fe1f-46e4-4491-9784-18639aed0393",
   "metadata": {},
   "source": [
    "And that's really what's going on here, so again, we took our documents, we split them, we embed them, and now they exist in this high dimensional space; we've taken our question, embedd it, projected it in that same space, and we just do a search around the question from nearby documents, and grab those ones that are close, and we can pick some number, for example we can say we want one, or two, or three, or N documents close to my question in this embedding space and there's a lot of really interesting methods that implement this very effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3ce26-ad12-4506-abbc-11cbbac8188a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG16.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 11</b>: Vectorstores Implement this for you</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17aebb-26b8-45e9-a7e6-97354b72457c",
   "metadata": {},
   "source": [
    "And we have a lot of really nice integrations to play with this general idea, so many different embedding models, many different indexes, lots of document loaders, and lots of splitters that can be kind of recombined to test different ways of doing this indexing retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5e005-c06c-4fc2-a0da-8676b3c54f0f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG17.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 12</b>: LangChain has many integrations to support this</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d4a13-c4ce-49c1-9397-0cacbd9e6e15",
   "metadata": {},
   "source": [
    "Now I'll show you a bit of code walkthrough. Here, in the slide we actually showed kind of that notion of search in that 3D space. A nice parameter to think about in building your retriever is **K**. So, **K** tells you the number of nearby neighbors to fetch when you do that retrieval process and we talked about in that 3D space. Do I want one nearby neighbor? or two? or three? so here we can specify K = 1 for example. Now we are building our index, so we're taking every split, embeding it, sroting it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e157b8-33c2-4966-9fed-b9ebdf333741",
   "metadata": {},
   "source": [
    "### Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d79e38-991b-4f2a-baa9-2f661ed055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                   embedding=OpanAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5fc68-f819-4458-89ee-5b35c67d1355",
   "metadata": {},
   "source": [
    "Now, what's nice is that I asked a question: \"What is task decomposition?\" this is related to the blog post, and I'm going to run 'get_relevant_documents':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ec7d8-4e9b-4be5-b19f-25d9c8424374",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e0038-4179-4826-9de8-61d911ea6996",
   "metadata": {},
   "source": [
    "So, I run that and now, how many documents do i get back? I get one as expected upon **K** = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382668be-c74c-4b68-9e2f-c902c63d15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d13fe8-f1aa-4c48-bbf5-75ff36a23262",
   "metadata": {},
   "source": [
    "So, this retrieved document should be related to my question. Here is when we can go to LangSmith and we can look at our retriever where we can see our question and the document we got back. (Pages 19-20 of word document). This document pertains to task decomposition in particular, and it kind of lays out a number of different approaches that can be used to do that. You can implement this by using KNN (K-Nearest-Neighbors) search really easily  just using a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a3c15-e9ec-40fb-868c-4c457eeab258",
   "metadata": {},
   "source": [
    "### <a id=\"generation\"></a>Generation\n",
    "An important consideration in Generation is really what’s happening is we’re taking the documents you retrieve and we’re stuffing them into the LLM context window. So if we walk back through the process, we take documents, we split them for convenience or embedding, we then embed each split, and we store that in a Vector Store as this kind of easily searchable numerical representation or vector, and we take a question, embed it to produce a numerical representation, we can then search, for example, using something like KNN in this kind of high dimensional space for documents that are similar to our question based on their proximity or location in this space (as this example of toy 3D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a83274-b0d6-4e3b-bed6-23378251798c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG18.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 13</b>: Adding Docs to Context Window</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd2679-e743-4a50-9c11-76ba849fd7c1",
   "metadata": {},
   "source": [
    "Now we’ve recovered relevant splits to our question, we pack them into the context window, and we produce our answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35256619-f474-4e5d-b335-c3182790edb5",
   "metadata": {},
   "source": [
    "Now, this introduces the notion of a Prompt. A **Prompt** is a placeholder that has, for example, in our own case, **keys**, and those keys can be like **context** and **question**. So, they are basically kind of buckets in which we're going to take those retrieved documents and slot them in it, and we'ra going to take our question and also slot in it. If you walk through this flow you can see that we can build a dictionary from our retrieved documents and from our question, and then we can basically populate our prompt template with the values from the dictionary, and then becomes a prompt value which can be passed to the LLM like a chat model, resulting in chat messages which we then parse into a string and get our answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de557f47-37a6-4164-9d4a-69ec812e85a6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG19.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 14</b>: Connecting Retrieval with LLMs via Prompt</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc74eb9-5d75-4d30-89a3-6c24dac96a71",
   "metadata": {},
   "source": [
    "Now, let's walk through that in code. Here's the Generation bit, and you can see here it's defines something new, this is a **Prompt Template** and my Prompt Template is something really simple, it's just going to say: \"Answer the following Question based on this Context\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55680552-0869-4cd9-bff3-2af6af08a78a",
   "metadata": {},
   "source": [
    "### Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d13879-87ec-4c1a-8a71-e12f755e707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.promps import ChatPromptTemplate\n",
    "\n",
    "#Prompt \n",
    "template = \"\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f1c2f-51c6-4ba8-bdb2-6a3148f7ddd5",
   "metadata": {},
   "source": [
    "Let's define the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332d2c64-5ccb-4c85-b7e4-a48b488c7b64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# LLM\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147ded9-6d36-47b2-a3ee-4b2b18e7690a",
   "metadata": {},
   "source": [
    "Now this introduces the notion of Chain, so, in LangChain we have an expression language called LCEL (LangChain Expression Language) which lets you really easily compose things like prompts, LLMs, parsers, retrievers and other things. But the very simple idea here is just let's take our prompt which we defined right before and connect it to a LLM which was defined before in this chain. So, there's our chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0db7e-5135-4ec8-a354-46f1a1c42397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5aba1-a497-4f1a-aabd-c76946eb8ebf",
   "metadata": {},
   "source": [
    "Now, all we are doing is invoking that chain, so, every LangChain Expression Langage has a few common methods like 'invoke', and in this case, we invoke it with a dict, so, context and question that map to the expected keys in our template. It is going to execute the chain and get our answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f48d50-1785-41b4-ae4c-077792777aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "chain.invoke({\"context\":docs, \"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a4eba-c984-4658-b805-3b5b3ba74722",
   "metadata": {},
   "source": [
    "We can go to LangSmith now and here we should see a very simple runnable sequence. We should see our document, our output, our prompt that says \"answer the following question based on the context\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f06bf-e3dd-47d5-8f29-67a9b7b8b5ed",
   "metadata": {},
   "source": [
    "There's a lot other options for RAG prompts. Next is a popular prompt just with a little bit more detail, but the intuition is the same. The prompt is: \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question, if you don't know the answer, just say tou don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88d783-bb0f-4d33-b168-6251e4595194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983acc0-f909-4575-be02-d30ec6ef13f0",
   "metadata": {},
   "source": [
    "You're passing in documents, you are asking them to reason about the documents, given a question producing an answer, and now I'm going to define a RAG Chain which will automatically do the retrieval for us, and all I have to do is specify: \"Here is my retriever which we defined before. Here is our question which we invoke with. The question gets passed through the key question in our dictionary, and it automatically will trigger the retriever which will return documents which which get passed into our context\" and now this is all automated for us. Now we pass that chain which is auto-populated into our prompt, LLM, auto-parser, now is invoked, and that should all just run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439a155-84ea-4d97-ad44-974fc7fa459e",
   "metadata": {},
   "source": [
    "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54720023-dd59-4c07-af38-d758f1432aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassThrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassThrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd10fe8-422a-4a01-8c5e-c05c4d1cf505",
   "metadata": {},
   "source": [
    "And great, we get an answer and we can look at the trace and see everything that happened by using LangSmith. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35613b6-919f-4e9a-b674-a6e56019470e",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64c09f-2a3c-4244-bab2-096ad7d47fda",
   "metadata": {},
   "source": [
    "### <a id=\"query-translation-multi-query\"></a>Query Translation (Multi Query)\n",
    "We’re going to talk about Query Translation, and specifically to cover the topic of multi-query. So, Query Translation sits at the first stage of an Advanced RAG pipeline, and the goal of Query Translation is really to take an input user question and translate it in some way in order to improve retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b585590-e7bc-431e-a339-9c1f2d81e6fd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 15</b>: Query Translation Stage</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba243c58-ab1d-4d12-ac44-4e2642b20e12",
   "metadata": {},
   "source": [
    "So, the problem statement is pretty intuitive, user queries can be ambiguous and because we’re typically doing some kind of semantic similarity search between the query and our documents, if the query is poorly written or ill posed, we won’t retrieve the proper documents from our index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4af35-afcd-45ad-bc5d-9274eb9278a8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG19.6.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 16</b>: Tweet about difficulties on embedding long documents and ambiguous queries</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396930e-88b7-4de6-a39e-aaa5b097747f",
   "metadata": {},
   "source": [
    "So, there’s a few approaches to attack this problem and you can group them in a few different ways. Here’s one way I like to think about it, a few approaches has involved Query Rewriting, so, taking a query and reframing it like writing from a different perspective, and that’s what we’re going to talk about a little bit here in depth, using approaches like Multi-Query, or RAG-Fusion which we’ll talk about in the next lesson.\n",
    "\n",
    "You can also do things like taking a question and breaking it down to make it less abstract into Sub-Questions and there’s a bunch of interesting papers focused on that like Least-to-Most from Google; you can also take the opposite approach of take a question to make it more abstract, and there’s actually approaches we’re going to talk about later in a future chapter called Step-Back Prompting that focuses on higher level question from the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235c156-b5cf-4e0c-b0a3-3c4f8a9ac3b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG20.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 17</b>: General Approaches to Transform Questions (Multi-Query)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f71d1c-74c0-4b99-b098-a50cd55ab276",
   "metadata": {},
   "source": [
    "So, the intuition for this multi-query approach is that we’re taking a question and we’re going to break it down into a few differently worded questions from different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d5e88-9037-42c0-b59e-ef93c48702e3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG21.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 18</b>: Transform a Question into Multiple Perspectives</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7dde4-4371-4dec-9568-23d9753917e7",
   "metadata": {},
   "source": [
    "And the intuition here is simply that it is possible that the way a question is initially worded, once embedded, is not well aligned or in close proximity in this high-dimensional embedding space to a document that we want to retrieve that’s actually related, so the thinking is that by rewriting it in a few different ways you actually increase the likelihood of retrieving the document that you really want, because of nuances in the way that documents and questions are embedded, this kind of more shotgun approach of taking a question, fanning it out into a few different perspectives may improve and increase the reliability of retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6166c-53b5-45b9-a219-2eb244106ae8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG21.6.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 19</b>: Intuition: Improve Search</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967aec8-da4c-45e8-a76b-0109234928f0",
   "metadata": {},
   "source": [
    "And of course we can combine this with Retrieval, so we can take our fan out questions, do retrieval on each one, combine them in some way and perform RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78af7f-00d4-470f-8d53-a96ca98f1248",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG22.5 - Multi-Query - Query Translation.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 20</b>: Multi-Query Diagram - use this with parallelized retrieval</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4a286-e7cd-4e92-8ec7-e3e5556cbe62",
   "metadata": {},
   "source": [
    "Now, let’s go to our code. After installing the packages and setting the LangChain API Keys which we’ll see why that’s quite useful shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6597a-5993-4263-8daa-7c9feed16a73",
   "metadata": {},
   "source": [
    "Note: Next Environment Section is the same that previously written at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502f071-ed37-4b27-aa0a-90ef23b9f274",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79989322-c554-43c1-b9dd-2b3f70669ec3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
      "     -------------------------------------- 54.5/54.5 kB 471.0 kB/s eta 0:00:00\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.17-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (2.31.0)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (1.4.39)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (1.23.5)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langchain_community) (6.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.11.11-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.32\n",
      "  Downloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
      "     ------------------------------------- 412.7/412.7 kB 25.2 MB/s eta 0:00:00\n",
      "Collecting langsmith<0.4,>=0.1.125\n",
      "  Downloading langsmith-0.3.3-py3-none-any.whl (333 kB)\n",
      "     ------------------------------------- 333.0/333.0 kB 20.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Collecting openai<2.0.0,>=1.58.1\n",
      "  Using cached openai-1.60.2-py3-none-any.whl (456 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Collecting packaging<25,>=23.2\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-32.0.0-py2.py3-none-any.whl (2.0 MB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Using cached onnxruntime-1.20.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Collecting tqdm>=4.65.0\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting overrides>=7.3.1\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-5.1.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-3.11.0-py2.py3-none-any.whl (72 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Collecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.8/94.8 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting orjson>=3.9.12\n",
      "  Using cached orjson-3.10.15-cp310-cp310-win_amd64.whl (133 kB)\n",
      "Collecting chroma-hnswlib==0.7.6\n",
      "  Using cached chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting typing_extensions>=4.5.0\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting typer>=0.9.0\n",
      "  Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Collecting httpx>=0.27.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting build>=1.0.3\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Using cached grpcio-1.70.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Collecting tokenizers>=0.13.2\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Collecting pydantic>=1.9\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3\n",
      "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.2.1-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting pyproject_hooks\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0\n",
      "  Using cached starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2023.11.17)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.5.0)\n",
      "Requirement already satisfied: idna in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.8.2-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.2.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Collecting opentelemetry-proto==1.29.0\n",
      "  Using cached opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0\n",
      "  Using cached opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0\n",
      "  Using cached opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (2.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "     ------------------------------------- 464.1/464.1 kB 28.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Collecting websockets>=10.4\n",
      "  Using cached websockets-14.2-cp310-cp310-win_amd64.whl (164 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.0.4-cp310-cp310-win_amd64.whl (284 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.9.0)\n",
      "Collecting zipp>=3.20\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (0.4.3)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.2.1)\n",
      "Collecting pyreadline3\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Installing collected packages: zipp, websockets, typing_extensions, types-requests, tqdm, tenacity, shellingham, rsa, python-dotenv, pyreadline3, pyproject_hooks, pygments, protobuf, propcache, packaging, overrides, orjson, opentelemetry-util-http, oauthlib, mmh3, mdurl, jsonpointer, jiter, importlib-resources, httpx-sse, httptools, h11, grpcio, fsspec, frozenlist, exceptiongroup, distro, deprecated, chroma-hnswlib, cachetools, bcrypt, backoff, async-timeout, annotated-types, aiohappyeyeballs, uvicorn, typing-inspect, tiktoken, requests-toolbelt, requests-oauthlib, pydantic-core, posthog, opentelemetry-proto, multidict, marshmallow, markdown-it-py, langchainhub, jsonpatch, importlib-metadata, humanfriendly, huggingface-hub, httpcore, googleapis-common-protos, google-auth, build, asgiref, anyio, aiosignal, yarl, watchfiles, tokenizers, starlette, rich, pydantic, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, kubernetes, httpx, dataclasses-json, coloredlogs, typer, pydantic-settings, opentelemetry-semantic-conventions, openai, onnxruntime, langsmith, fastapi, aiohttp, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, opentelemetry-instrumentation-fastapi, langchain, langchain_community, chromadb\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.11.0\n",
      "    Uninstalling zipp-3.11.0:\n",
      "      Successfully uninstalled zipp-3.11.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'c:\\\\users\\\\daniel\\\\anaconda3\\\\scripts\\\\pygmentize.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -standard (c:\\users\\daniel\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf24db-450d-4174-bbe9-1d020db068b3",
   "metadata": {},
   "source": [
    "I’ve set a few API keys for LangSmith which are very useful for tracing which we’ll see shortly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd220f2-e295-41b1-9b5e-ea43e128f82a",
   "metadata": {},
   "source": [
    "(2) LangSmith\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9c7fe-3b81-41d8-9220-b92026e03018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = <your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0906914-cab7-4bcf-8af3-8d423f439316",
   "metadata": {},
   "source": [
    "### Part 5: Multi-Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb2223-b4b6-428c-a48d-ab25bfe03146",
   "metadata": {},
   "source": [
    "First I'm going to index this blog post on agents, load it, split it, and then index it in Chroma locally being this the Vector Store as we've done this previously. Now I have my index defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5685494-4e1e-4729-b8c8-d66eee65a662",
   "metadata": {},
   "source": [
    "Docs:\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
    "\n",
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a901f-33c5-42dc-9c40-b94150b1a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load Blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")        \n",
    "        )    \n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "#Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)\n",
    "\n",
    "#Make Splits \n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "#Index\n",
    "from langchain.text_splitter import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                   embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b072dd-f26e-4206-8ac9-bae3c76ce87b",
   "metadata": {},
   "source": [
    "Next, I'm going to define my prompt for Multi-Query, that's the template. We pass our prompt, then that to an LLM, parse it into a string and then split the string by new lines, and so, we'll get a list of questions of this chain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84d942-ca06-4b18-bce5-7bd6fd54caf9",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1dd5b5-fcde-48ac-8073-92550b6faf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\" You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7a651-b608-44dc-9a56-cc4c8dac84e9",
   "metadata": {},
   "source": [
    "That is our defined generate query chain, that's the sample input question. We're going to take that list and simply apply each question to retriever, so, we'll do retrieval per question. The function 'get_unique_union' is going to take the unique union of documents across all those retrievals. After running that we're going to get some set of questions or documents back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c22c4-4d09-4374-83da-f4f85d318ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents:list[list]):\n",
    "    \"\"\"Unique union of retrieved documents \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    #Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    #Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "#Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84068386-f713-40b9-b4f0-d0d03d9d26a3",
   "metadata": {},
   "source": [
    "If we go to LangSmith we can see what happened under the hood. Here we ran our initial chain to generate a set of reframed questions from our input. We can see a list of five retrievers given the next generated questions from the initial question \"What is task decomposition for LLM agents?\":\n",
    "\n",
    "- How do LLM agents perform task decomposition?\n",
    "- Can you explain the concept of task decomposition in LLM agents?\n",
    "- What are the methods used by LLM agents for task decomposition?\n",
    "- How does task decomposition work in the context of LLM agents?\n",
    "- What is the role of task decomposition in LLM agent's funtioning?\n",
    "\n",
    "For one of those questions we did an independent retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fabf9-0ae8-4ad4-baca-07a2d612e800",
   "metadata": {},
   "source": [
    "Now, let's come back to the code and show this working end to end. We are going to take that retrieval chain and pass it into context of our final RAG prompt, pass it to an LLM, and then parse the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e743b-b05c-4e78-b2ca-859879cc7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "#RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a874f-a14c-49e3-ab71-1a49a6927d5c",
   "metadata": {},
   "source": [
    "We can then go to LangSmith and see what happened under the Hood. Our final chain consisted on: taking our input question, breaking it out to 5 rephrased questions, doing a retrieval for every of those, taking the unique union of documents. Then, we can see our final LLM prompt answering the question based on the context. \n",
    "\n",
    "LangSmith can be used to investigate those intermediate questions that you generate in the question generation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6375ef-cabf-4a90-94ab-85e46d59e5dd",
   "metadata": {},
   "source": [
    "### <a id=\"query-translation-rag-fusion\"></a>Query Translation (RAG Fusion)\n",
    "\n",
    "In this first stage in an advanced RAG pipeline, we’re taking an input user question and we’re translating it in some way in order to improve retrieval. Now, we showed this general mapping of approaches previously, so, again, you have kind of rewriting, so you can take a question and break it down into differently worded different perspectives of the same question, so that’s rewriting; there’s sub-questions where you take a question, break it down into smaller problems, solve each one independently; and there’s Step-Back where you take a question and go more abstract where you ask a higher level question as a precondition to answer the user question, so those are approaches and we’re going to dig into one of the particular approaches for rewriting called RAG-Fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a60e5-f9b2-42ae-8470-06636fb0ec51",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG23.5.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 21</b>: General Approaches to Transform Queries - RAG-Fusion</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884acd7-59e3-4799-adbb-17339005f139",
   "metadata": {},
   "source": [
    "Now, this is really similar to what we just saw with multi-query, the difference being we actually apply a a clever ranking step of our retrieve documents, which you call Reciprocal Rank Fusion. That’s really the only difference, the input stage of thinking a question, breaking it out into a few kinds of differently worded questions, retrieval of each one, is all the same, and we’re going to see that in code here shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa9ef21-1938-4e8d-b88b-ebe2517f9f34",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"RAG24.5 - RAG-Fusion - Query Translation.png\" alt=\"Image\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"><b>Fig 22</b>: RAG-Fusion Diagram - Improve Search and Produce Consolidate Ranking</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2289e0-c148-4ae3-b366-239d482d762f",
   "metadata": {},
   "source": [
    "### Part 6: RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaacb81-8bd4-4e7d-bf24-42c550a7efd3",
   "metadata": {},
   "source": [
    "Docs:\n",
    "\n",
    "* https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb?ref=blog.langchain.dev\n",
    "\n",
    "Blog / repo: \n",
    "\n",
    "* https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be9705-2197-4fc7-8492-c1fb20ac09a2",
   "metadata": {},
   "source": [
    "The first thing you'll note is what our prompt is, so it looks really similar to the prompt we just saw with Multi-Query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c907b-9b15-466c-aed9-8ff7a42c0692",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc173eb1-8532-44b0-800b-dee666d2e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output {4 queries}:\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f01aa-2a71-46a1-8d73-5e7be316764e",
   "metadata": {},
   "source": [
    "Let's define our prompt and here is our query generation chain, again this looks like we just saw: we take our prompt, plug that into an LLM, and then basically parse by new lines, and that will basically split out these questions into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc362e7-9f9d-47d8-9be5-d4a757b3eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957b04a-9ea1-43e5-927d-5ad32bd9b588",
   "metadata": {},
   "source": [
    "Now, here's where the novelty comes in. Each time we do retrieval from one of those questions, we are going to get back a list of documents from our retriever, and so, we do it until we generate 4 answers based on our prompt that consisted on 4 questions, then, we get a list of lists. Reciprocal Rank Fusion is really well suited for this exact problem where we want to take this lists and build a single consolidated list. It is looking at the document in each list and aggregating them into a final output ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e095b-2dae-419e-8f7d-bd8bf3cd1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[doc_str] = 0\n",
    "                        #Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "                           , k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "    and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "    \n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # if the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            #Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1/ (rank + k)\n",
    "            \n",
    "    #Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3b714-5e0a-4680-93e7-87be34e60264",
   "metadata": {},
   "source": [
    "Now we can go to LangSmith and see what is happening in some detail. Here we can see our prompt to your helpful assistant that generate multiple search queries based on a single input, and also our search queries that were these:\n",
    "\n",
    "- How does task decomposition work for LLM agents?\n",
    "- Benefits of task decomposition in LLM agents?\n",
    "- Examples of task decomposition techniques for LLLM agents.\n",
    "- Challenges in implementing task decomposition for LLM agents.\n",
    "\n",
    "And for each one of the outputs we have a retrieval, in total 4 retrievals, and then, those retrievals simply go into the previous function explained and our corresponding ranking to a final list of six unique ranked documents. So, let's actually put all that together into a full RAG chain that's going to run Retrieval, return that final list of ranked documents and pass it to our context, pass through our question, send to a RAG prompt, pass it to an LLM, parse it to an output and let's run all that together and see that working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793a4d0-4605-481c-969e-83df7d8e5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "#RAG \n",
    "template = \"\"\"Answer the following question based on this context: \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec93f2-bac0-4659-84b6-d4e508edc9ed",
   "metadata": {},
   "source": [
    "If we look in LangSmith, it is possible to see those four questions, the retrievers for each question, and our final RAG prompt plumed through the final list of ranked four questions. \n",
    "\n",
    "So, this can be really convenient, particularly if we're operating across different vector stores, or we want to do retrieval across a large number of differently worded questions, this Reciprocal Rank Fusion step is really nice if for example we wanted to only take the top three documents or something. It can be really nice to build that consolidated ranking across all these independent retrievals, then pass that to an LLM for the final generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d8d6f-2b90-42af-9ece-f2074137bbea",
   "metadata": {},
   "source": [
    "### Codeium Detailed Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a6015-f80f-40b1-b588-b5b569b60f04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Indexing\n",
    "\n",
    "The code starts by loading a blog post from a URL using the WebBaseLoader class from langchain_community.document_loaders. The loader is configured to parse only the content, title, and header of the post.\n",
    "\n",
    "The loaded blog post is then split into smaller chunks using the RecursiveCharacterTextSplitter class from langchain.text_splitter. The splitter is configured to split the text into chunks of 300 characters with an overlap of 50 characters.\n",
    "\n",
    "The resulting chunks are then indexed using the Chroma class from langchain_community.vectorstores. The Chroma class is a vector store that allows for efficient similarity searches. The indexing process involves converting the chunks into embeddings using the OpenAIEmbeddings class from langchain.text_splitter.\n",
    "\n",
    "#### RAG-Fusion\n",
    "\n",
    "The code then defines a prompt template for generating multiple search queries based on a single input query. The prompt template is defined using the ChatPromptTemplate class from langchain.prompts.\n",
    "\n",
    "The generate_queries function is defined as a pipeline that takes the prompt template, generates queries using the ChatOpenAI class from langchain_openai, and parses the output using the StrOutputParser class from langchain_core.output_parsers.\n",
    "\n",
    "Reciprocal Rank Fusion\n",
    "\n",
    "The reciprocal_rank_fusion function is defined as a function that takes multiple lists of ranked documents and an optional parameter k used in the RRF formula.\n",
    "\n",
    "Here's a step-by-step explanation of the reciprocal_rank_fusion function:\n",
    "\n",
    "1. Initialize an empty dictionary fused_scores to hold the fused scores for each unique document.\n",
    "2. Iterate through each list of ranked documents in the results parameter.\n",
    "3. For each document in the list, convert the document to a string format using the dumps function from langchain.load. This is done to use the document as a key in the fused_scores dictionary.\n",
    "4. Check if the document is already in the fused_scores dictionary. If not, add it with an initial score of 0.\n",
    "5. Retrieve the current score of the document from the fused_scores dictionary.\n",
    "6. Update the score of the document using the RRF formula: 1 / (rank + k), where rank is the position of the document in the list and k is the optional parameter.\n",
    "7. Repeat steps 3-6 for each document in each list.\n",
    "8. Sort the documents based on their fused scores in descending order to get the final reranked results.\n",
    "9. Return the reranked results as a list of tuples, where each tuple contains the document and its score.\n",
    "\n",
    "#### Retrieval Chain\n",
    "\n",
    "The retrieval_chain_rag_fusion function is defined as a pipeline that takes the generate_queries function, maps the output to the retriever function, and applies the reciprocal_rank_fusion function.\n",
    "\n",
    "The docs variable is assigned the result of invoking the retrieval_chain_rag_fusion function with a question as input.\n",
    "\n",
    "#### RAG\n",
    "\n",
    "The code then defines a prompt template for answering a question based on a given context. The prompt template is defined using the ChatPromptTemplate class from langchain.prompts.\n",
    "\n",
    "The final_rag_chain function is defined as a pipeline that takes the retrieval_chain_rag_fusion function, the prompt template, and the llm function (not defined in the code snippet).\n",
    "\n",
    "The final_rag_chain function is invoked with a question as input, and the result is not assigned to any variable.\n",
    "\n",
    "#### Important Considerations\n",
    "\n",
    "1. The reciprocal_rank_fusion function assumes that the documents can be serialized to JSON format using the dumps function from langchain.load.\n",
    "2. The k parameter in the RRF formula is an optional parameter that can be adjusted based on your specific needs. A larger value of k can give more weight to the documents with higher ranks.\n",
    "3. The code uses the ChatOpenAI class from langchain_openai to generate search queries based on a single input query.\n",
    "4. The llm function is not defined in the code snippet, so it's assumed to be defined elsewhere in your codebase.\n",
    "5. The code uses the StrOutputParser class from langchain_core.output_parsers to parse the output of the chat prompt into a string.\n",
    "\n",
    "#### Possible Changes or Additions\n",
    "\n",
    "1. Modify the k parameter in the RRF formula to adjust the weight given to different ranks.\n",
    "2. Change the bs_kwargs dictionary in the WebBaseLoader class to load different web pages or specify different parsing rules.\n",
    "3. Modify the chunk_size and chunk_overlap parameters in the RecursiveCharacterTextSplitter class to adjust the splitting of the text.\n",
    "4. Change the embedding parameter in the Chroma.from_documents function to use different embeddings or pre-trained models.\n",
    "5. Modify the template in the ChatPromptTemplate class to change the prompt format or add additional information.\n",
    "6. Replace the llm function with a different language model or chatbot implementation.\n",
    "7. Add additional processing steps to the retrieval_chain_rag_fusion pipeline, such as filtering or ranking the results.\n",
    "8. Use a different algorithm for ranking the documents, such as BM25 or TF-IDF.\n",
    "9. Experiment with different values for the temperature parameter in the ChatOpenAI class to adjust the level of randomness in the generated queries.\n",
    "10. Use a different output parser, such as JSONOutputParser, to parse the output of the chat prompt into a different format.\n",
    "\n",
    "#### Considerations\n",
    "\n",
    "1. The reciprocal_rank_fusion function assumes that the documents can be serialized to JSON format using the dumps function from langchain.load. If this is not the case, you may need to modify the function to use a different serialization method.\n",
    "2. The k parameter in the RRF formula is an optional parameter that can be adjusted based on your specific needs. A larger value of k can give more weight to the documents with higher ranks.\n",
    "3. The code uses the ChatOpenAI class from langchain_openai to generate search queries based on a single input query. If you want to use a different language model or chatbot implementation, you will need to replace this class with a different one.\n",
    "4. The llm function is not defined in the code snippet, so it's assumed to be defined elsewhere in your codebase. If you want to use a different language model or chatbot implementation, you will need to define this function accordingly.\n",
    "5. The code uses the StrOutputParser class from langchain_core.output_parsers to parse the output of the chat prompt into a string. If you want to parse the output into a different format, you will need to use a different output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f173392-f45f-4073-af3a-b650ac0ba471",
   "metadata": {},
   "source": [
    "### <a id=\"query-translation-decomposition\"></a>Query Translation (Decomposition)\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06aa9ca-85ce-4990-8438-1af9cb9e14ae",
   "metadata": {},
   "source": [
    "### <a id=\"query-translation-step-back\"></a>Query Translation (Step Back)\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941dfeb-b124-4a06-9e52-65d3a1d85ba5",
   "metadata": {},
   "source": [
    "### <a id=\"query-translation-hyde\"></a>Query Translation (HyDE)\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce6fd6-a6a7-4b80-a568-4bb8e18f14ee",
   "metadata": {},
   "source": [
    "### <a id=\"routing\"></a>Routing\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554227b-bea0-4f54-9b19-888fbfd68701",
   "metadata": {},
   "source": [
    "### <a id=\"query-construction\"></a>Query Construction\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39978c6e-e553-41bc-9536-d7ffbbe03e42",
   "metadata": {},
   "source": [
    "### <a id=\"indexing-multi-representation\"></a>Indexing (Multi Representation)\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728f075-c526-4e7a-8e46-40da95b88cc2",
   "metadata": {},
   "source": [
    "### <a id=\"indexing-raptor\"></a>Indexing (RAPTOR)\n",
    "Your content here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36607780-fb47-4147-8958-72d32a9402ac",
   "metadata": {},
   "source": [
    "### <a id=\"indexing-colbert\"></a>Indexing (ColBERT)\n",
    "Your content here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4f241-72cb-45b0-b98e-4cb987b752a5",
   "metadata": {},
   "source": [
    "### <a id=\"crag\"></a>CRAG\n",
    "Your content here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a821ed-5926-47ae-a942-6f69d3d2a81b",
   "metadata": {},
   "source": [
    "### <a id=\"adaptive-rag\"></a>Adaptive RAG\n",
    "Your content here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcfce6b-5c43-47ec-81fa-94ae0d530c29",
   "metadata": {},
   "source": [
    "### <a id=\"the-future-of-rag\"></a>The Future of RAG\n",
    "Your content here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18b2ec-70aa-47f1-a057-9cd537503af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
